{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this unit you will learn:\n",
    "1. Limitations of accuracy as a classification metric\n",
    "2. Importance of understanding the target distribution\n",
    "3. Strategies for optimizing on Imbalanced Datasets\n",
    "    1. Class weight\n",
    "4. Confusion Matrix\n",
    "5. Beyond accuracy\n",
    "    1. Recall, Precision and f1-score\n",
    "6. Threshold Analysis\n",
    "7. Threshold-independent metrics\n",
    "    1. ROC and Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data','insurance_claims.csv'),index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "# train validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = lambda preds, y_true: sum(preds == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(preds,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/big_brain.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems ok right? We got 96% accuracy so the problem is solved..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/wrong.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's dig deeper\n",
    "\n",
    "Let's say i have a dumb classifier that always predicts 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_classifier = lambda X: np.array([0] * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to \"train\" it. Let's just evaluate it on the validation set\n",
    "preds = dumb_classifier(X_val)\n",
    "\n",
    "accuracy_score(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that, we got a suspiciously high score which is also the same score as with the LogisticRegression. Can you guess what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing ML development one of the things you should understand well is your target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().to_frame('Value').assign(perc = lambda x: x/x.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive class is very rare! \n",
    "\n",
    "Our model is trying to optimize for the overall error, which is easy to do in imbalanced datasets by always predicting zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigation measure for imbalanced datasets: Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sklearn algorithms can optimize taking into account class proportions. This means that an error on the positive class can be as costly as you want for the optimization so that it has more weight on model training to compensate for the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = y_train.value_counts() / len(y_train)\n",
    "weight_positive = dist[0] / dist[1]\n",
    "clf = LogisticRegression(class_weight={0:1,1:weight_positive})\n",
    "\n",
    "# the above is equivalent to this:\n",
    "#clf = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_val)\n",
    "round(accuracy_score(preds, y_val),2)\n",
    "pd.Series(preds).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the model is predicting positive class sometimes now!\n",
    "\n",
    "Accuracy went down though, but we will come back to this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great way to look at a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/confusion.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(6,4));\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False);  # fmt='g' is for integer formatting\n",
    "    plt.xlabel('Predicted Labels');\n",
    "    plt.ylabel('True Labels');\n",
    "    plt.title('Confusion Matrix');\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our model performance in a very raw format: confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, preds)\n",
    "\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this data as frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_freq = (cm / cm.sum(axis=1)[:, np.newaxis]).round(2)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "1. When the class is negative, our model correctly predicts negative 63% of the time, and fails the remaining 37%\n",
    "2. When the class is positive, our model correctly predicts positive 56% of the time, and fails the remaining 44%.\n",
    "\n",
    "\n",
    "So when the class is positive, it is a coinflip between whether our model will predict correctly or not. But does that mean our model is no better than a random guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would a random model (coin flip) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a random model\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "random_model = lambda X: np.random.randint(0,2,len(X))\n",
    "\n",
    "random_preds = random_model(X_val)\n",
    "\n",
    "pd.Series(random_preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cm = confusion_matrix(y_val, random_preds)\n",
    "\n",
    "plot_confusion_matrix(random_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A truly random model will have about 50%/50% distribution on the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going beyond accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you *recall*, our most recent model had the following confusion matrix and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(preds,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy can be a very bad estimator of performance on imbalanced datasets. But more than that, it leaves many questions unanswered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many true labels did you find out of the total true labels that exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I have 1000 patients and 100 of them have cancer. If my model finds correctly 30 patients with cancer, then it's recall is 30/100 = 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall measures how many True positives you found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/recall.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the denominador, TP + FN is the total number of positives (P) that exist : All that you've guessed correctly (TP) + all that you incorrectly labeled as Negatives (FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: How can we trick the recall metric in a similar way we did with accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What recall does our model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score = lambda preds, y_true: sum((preds == 1) & (y_true == 1)) / sum(y_true == 1)\n",
    "\n",
    "recall_score(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our model finds 56% of the positive class. If we were talking about cancer patients, it can find 56% of all cancer patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Much like yin and yang, precision complements recall extremely well\n",
    "\n",
    "> Everytime you predict positive, how many times are you actually right?\n",
    "\n",
    "\n",
    "Let's take an example:\n",
    "\n",
    "I have 1000 patients where 100 have cancer. I predict 200 of them to have cancer. I was right on 50 of them, but 150 of those i wasn't (False positives). My precision is 50/200 = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What precision does our model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score = lambda preds, y_true: sum((preds == 1) & (y_true == 1)) / sum(preds == 1)\n",
    "\n",
    "precision_score(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5% only! So everytime the model predicts a positive class, it is correct only 5% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How can one optimize for precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to that is: have the model predict positive only when it is very certain about its prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X_val)[:,1]\n",
    "probas[:5] # show only the first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you remember classifier models output probabilities. And we obtain predictions by passsing these probabilities through a threshold that, by default, is 0.5. So anything above 0.5 probability we classify as class 1, and the remaining as class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/threshold.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a bunch of thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = np.arange(0.1,1,step=0.1)\n",
    "ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for decision_threshold in ths:\n",
    "    th_preds = (probas > decision_threshold).astype(int)\n",
    "\n",
    "    recall = recall_score(th_preds, y_val)\n",
    "    precision = precision_score(th_preds, y_val)\n",
    "    \n",
    "    results.append({\"decision_threshold\":decision_threshold,\"recall\":recall,\"precision\":precision})\n",
    "\n",
    "results = pd.DataFrame(results).set_index('decision_threshold')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, because our model isn't very good, we didn't see a drastic improvement in precision (although it doubles from 0.5 threshold to 0.8), but if you take a look at recall, it increases by 30% (percentual points) just from lowering the decision threshold from 0.5 to 0.4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consequences of having a high decision threshold**\n",
    "\n",
    "If you only make a prediction when the model is absolutely certain:\n",
    "- **Your precision will be higher** because you are only classifying as positive when you are certain (i.e. you are decreasing your FP)\n",
    "- **Your recall will be lower** because you are missing out on a lot of positives (i.e. you are increasing your FN: False Negatives)\n",
    "> you decrease your FPs (good) at the cost of increasing False Negatives (bad)\n",
    "\n",
    "**Consequences of having a low decision threshold**\n",
    "\n",
    "If you make positive predictions with a relatively low decision threshold:\n",
    "- **Your recall will be higher**. Since you are classifying more things as positive, you are more likely to find more Positives than you were before (you are increasing your TP: True Positives)\n",
    "- **Your precision will be lower** because you are increasing your Positives indiscriminately and in an imbalanced dataset, you are more likely to include False Positives doing that than True Positives (i.e. you are increasing your FP).\n",
    "> you increase your TPs (good) at the cost of FP (bad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 - score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end what should you optimize for? If we go full precision-mode we are sacrificing recall and if we go full-recall model we sacrifice precision.\n",
    "\n",
    "This is where F1 score comes in handy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/F1-Score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the complex formula, this is what's called an [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean). It is very similar to a simple mean/average between two numbers, except the lower number will push the average a bit more towards itself than you would expect.\n",
    "\n",
    "Why is this useful? Usually in ML systems we want a good tradeoff between recall and precision. We are not really interested in extremes where the precision is super good and the recall is super bad. For these cases it is generally preferable to have a good balance between the two. Let's look at an example:\n",
    "\n",
    "\n",
    "Take $H(r,p)$ to be the harmonic mean between a recall and precision scores, and $M(r,p)$ to be a normal mean between the two scores.\n",
    "\n",
    "Scenario 1:\n",
    "\n",
    "r = 1.0, p = 0.0\n",
    "- $H(r,p) = 0$\n",
    "- $M(r,p) = 0.5$\n",
    "\n",
    "Scenario 2:\n",
    "\n",
    "r = 0.9, p = 0.3\n",
    "\n",
    "- $H(r,p) = 0.45$\n",
    "- $M(r,p) = 0.60$\n",
    "\n",
    "Scenario 3:\n",
    "\n",
    "r = 0.7\n",
    "p = 0.9\n",
    "\n",
    "- $H(r,p) = 0.79$\n",
    "- $M(r,p) = 0.80$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = lambda preds, y_true: 2 * precision_score(preds, y_true) * recall_score(preds, y_true) / (precision_score(preds, y_true) + recall_score(preds, y_true))\n",
    "\n",
    "\n",
    "f1_score(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Independent metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One way to look at it is that a model is very good if it can order the negatives and positives well along the probability outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/classification_threshold.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect model would place all red circles on the left side and all blue ones on the right side. But in reality we always get something messy like the image above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it would be nice to summarize a model's performance without having to decide on a threshold, and thankfully we have 2 measures for that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC (Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each intersection record the:\n",
    "-  **True Positive Rate** (recall) and\n",
    "- **False Positive Rate** (probability of false positive/alarm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/roc-1.webp\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random model would have always the characteristic of the dashed blue line above. And a perfect model is one where its ROC curve is highest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/Roc_curve.svg.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize a ROC performance by taking it's **Area Under the Curve (AUC)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random model will have an AUC-ROC of 0.5, and a perfect model has AUC-ROC of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, probas)\n",
    "\n",
    "# plot the curve\n",
    "plt.figure(figsize=(6,4));\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve');\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); # random model\n",
    "plt.xlabel('False Positive Rate');\n",
    "plt.ylabel('True Positive Rate');\n",
    "plt.title('ROC curve');\n",
    "plt.legend(loc=\"lower right\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a model slightly better than random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall Curve and Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the precision-recall curve is another way to evaluate models without using a threshold.\n",
    "\n",
    "We still compute the checkpoints at each intersection but instead of recording the FPR and the TPR, we record the Precision and the Recall (TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/Precision-Recall-Curve-of-a-Logistic-Regression-Model-and-a-No-Skill-Classifier2.webp\"  width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random model is one that has a curve with a value equal to the prevalence of the data (% of positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve is called the **average-precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_val, probas)\n",
    "\n",
    "# plot the curve\n",
    "plt.figure(figsize=(6,4));\n",
    "plt.plot(recalls, precisions, color='darkorange', lw=2, label='Precision-Recall curve');\n",
    "#plt.plot([0, 1], [y_val.mean(), y_val.mean()], color='navy', lw=2, linestyle='--'); # random model\n",
    "plt.xlabel('Recall');\n",
    "plt.ylabel('Precision');\n",
    "plt.title('Precision-Recall curve');\n",
    "plt.legend(loc=\"lower right\");\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_val, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use either one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a balanced dataset, using AUC-ROC is fine. But in imbalanced datasets you should use the Average Precision.\n",
    "\n",
    "In a very imbalanced dataset, you have a lot of Negatives. In ROC, if you try to increase recall by lowering the threshold, you increase the FPR. That is bad, however the FPR won't increase by much because the number of negatives is high (look at the denominator of the formula)\n",
    "\n",
    "Average Precision is much more unforgiving, because if you lower the threshold to increase recall, your precision metric can be easily (negatively) affected as a result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional references\n",
    "\n",
    "[A really cool article on area under the curve](https://sinyi-chou.github.io/classification-auc/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
