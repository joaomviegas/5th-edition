{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model with a GRU encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement a sequence to sequence model with RNN encoder and decoder, together with an attention mechanism.\n",
    "\n",
    "### Encoder\n",
    "As before, the encoder will take a source sentence as input and will encode it into a single vector (also known as a context vector or a latent vector) which will be passed to the decoder. The decoder will then use this context vector to generate a new sequence (in our case, a translation of the source sentence). However, we will use the hidden states from each step to calculate the attention weights.\n",
    "\n",
    "So, given a sequential input sentence $X = \\{x_1, x_2, ..., x_T\\}$, we want to encode it into a single vector $z$. At each step we have the hidden state $h_t$:\n",
    "<br>\n",
    "<br>\n",
    "$$h_t = \\text{Encoder}(e(x_t), h_{t-1}),$$\n",
    "<br>\n",
    "where $e(x_t)$ is the embedding of the current token $x_t$. This means that in practice, the $z$ vector will actually be $h_T$, the last hidden state of the RNN.\n",
    "\n",
    "\n",
    "### Attention \n",
    "\n",
    "The attention will be calculated using the hidden states of the encoder and the last hidden decoder state.  We will use the dot product between the last hidden decoder state $s_{t-1}$ and the hidden states of the encoder $h_t$ to calculate the attention weights, and apply a softmax to normalise the weights:\n",
    "<br>\n",
    "<br>\n",
    "$$\\alpha_t = \\text{softmax}(s_{t-1}^T h_t),$$\n",
    "\n",
    "where $\\alpha_t$ is the attention weight for the hidden state $h_t$. We will use the attention weights to calculate the context vector $c_t$:\n",
    "<br>\n",
    "<br>\n",
    "$$c_t = \\sum_{i=1}^T \\alpha_{t,i} h_i,$$\n",
    "\n",
    "where $T$ is the length of the input sequence. The context vector $c_t$ will be concatenated with the hidden state $s_{t-1}$ and the embedding of the previous token $e(y_{t-1})$ to predict the next hidden state $s_t$:\n",
    "<br>\n",
    "<br>\n",
    "$$s_t = \\text{Decoder}(e(y_{t-1}), s_{t-1}, c_t).$$\n",
    "\n",
    "Finally, we use the hidden state $s_t$ to predict the next token $\\hat{y}_{t+1}$. We do this until we predict an end-of-sentence token, or we reach a maximum length of the sequence. \n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will the dataset named \"VanessaSchenkel/translation-en-pt\", available in HuggingFace's datasets library. This dataset contains pairs of sentences in English and Portuguese. We will use this dataset to train our model to translate from English to Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe519beb47d40858be63590d0204cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d109005caea4e6699d5edaa3267aabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20e5ade5f92482a928054e70104e7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/59.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7c467a7e1540dabea386557c07d7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a98d185d714f99a5660f294cbba21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "main_data = load_dataset(\"VanessaSchenkel/translation-en-pt\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 260482\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains only a train split, so we will split it into train and validation sets. We will use 80% of the data for training and 20% for validation. \n",
    "\n",
    "Like before we first need to pre-process the data and tokenize all examples, encode them into integers and create dataloaders to iterate over the batches. \n",
    "\n",
    "We will use the same tokenizer as before, but we will need to add the special tokens \"\\<eos\\>\" (end of sentence) and \"\\<sos\\>\" (start of sentence) to the vocabulary. \n",
    "\n",
    "We start by building the two vocabulary, one for the source language (English) and one for the target language (Portuguese).\n",
    "\n",
    "Note: we limit the total number of exemples to 50000 to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size:  11461\n",
      "Portuguese vocabulary size:  17554\n",
      "\n",
      "Maximum length of English sentences:  15\n",
      "Maximum length of Portuguese sentences:  15\n"
     ]
    }
   ],
   "source": [
    " # For this example we will keep punctuation and capital letters, meaning that can use directly the word_tokenize function from nltk\n",
    "# Also, we will not remove stopwords or rare words, since they can be important for the translation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# note: skip the example with id 199351, since it is a very long sentence\n",
    "english_tokens= []\n",
    "portuguese_tokens = []\n",
    "for d in main_data['train']:\n",
    "    eng_tokens = word_tokenize(d['translation']['english'].lower())\n",
    "    pt_tokens = word_tokenize(d['translation']['portuguese'].lower())\n",
    "    if len(eng_tokens) > 15 or len(pt_tokens) > 15:\n",
    "        continue\n",
    "    english_tokens.append(eng_tokens)\n",
    "    portuguese_tokens.append(pt_tokens)\n",
    "    if len(english_tokens) == 50000:\n",
    "        break\n",
    "\n",
    "# Is 15 but let's get the maximum length of the sentences. We will use this to pad the sentences\n",
    "max_len_english = max([len(s) for s in english_tokens])\n",
    "max_len_portuguese = max([len(s) for s in portuguese_tokens])\n",
    "\n",
    "# Ok, now we can get the unique tokens for each language\n",
    "unique_english_tokens = sorted(list(set([tk for s in english_tokens for tk in s])))\n",
    "unique_portuguese_tokens = sorted(list(set([tk for s in portuguese_tokens for tk in s])))\n",
    "\n",
    "print(\"English vocabulary size: \", len(unique_english_tokens))\n",
    "print(\"Portuguese vocabulary size: \", len(unique_portuguese_tokens))\n",
    "print(\"\")\n",
    "print(\"Maximum length of English sentences: \", max_len_english)\n",
    "print(\"Maximum length of Portuguese sentences: \", max_len_portuguese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', \"'s\", 'try', 'something', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_english_tokens = ['<pad>','<sos>', '<eos>'] + unique_english_tokens\n",
    "tokeng2id = {t: i for i, t in enumerate(unique_english_tokens)}\n",
    "id2tokeng = {i: t for t, i in tokeng2id.items()}\n",
    "\n",
    "unique_portuguese_tokens = ['<pad>','<sos>', '<eos>'] + unique_portuguese_tokens\n",
    "tokpt2id = {t: i for i, t in enumerate(unique_portuguese_tokens)}\n",
    "id2tokpt = {i: t for t, i in tokpt2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokeng2id[\"<pad>\"])\n",
    "print(tokeng2id[\"<sos>\"])\n",
    "print(tokeng2id[\"<eos>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simpler let's add the special tokens manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokens_ids = [[1]+[tokeng2id[t] for t in s]+[2] for s in english_tokens]\n",
    "portuguese_tokens_ids = [[1]+[tokpt2id[t] for t in s]+[2] for s in portuguese_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pad each sentence to the maximum length of the batch. This means that if the maximum length of the batch is 50, all sentences will be padded to length 50. \n",
    "\n",
    "We will the english sentences to the left because we want the final token to be the \\<eos\\>, and we will pad the portuguese sentences to the right because we want the first token to be the \\<sos\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_length = 500, pad_direction = 'left'):\n",
    "    if pad_direction == 'left':\n",
    "        return seq[:max_length] if len(seq) > max_length else [0] * (max_length - len(seq)) + seq\n",
    "    elif pad_direction == 'right':\n",
    "        return seq[:max_length] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "    else:\n",
    "        raise ValueError(\"pad_direction must be either 'left' or 'right'\")\n",
    "\n",
    "\n",
    "english_tokens_ids = [pad_sequence(seq, max_length=17,pad_direction='left') for seq in english_tokens_ids]\n",
    "portuguese_tokens_ids = [pad_sequence(seq, max_length=17,pad_direction='right') for seq in portuguese_tokens_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6057, 24, 10606, 9529, 29, 2]\n",
      "[1, 16811, 16005, 1005, 3647, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(english_tokens_ids[0])\n",
    "print(portuguese_tokens_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "\n",
    "Ok, we are now ready to create the dataset and the dataloaders. We will use the same batch size as before (32).\n",
    "Also, before we need to do the split between train and validation sets. We will use 80% of the data for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  40000\n",
      "Size of validation set:  10000\n"
     ]
    }
   ],
   "source": [
    "# We assume the data is already randomly shuffled\n",
    "train_size = int(len(english_tokens_ids) * 0.8)\n",
    "\n",
    "train_en = english_tokens_ids[:train_size]\n",
    "train_pt = portuguese_tokens_ids[:train_size]\n",
    "\n",
    "val_en = english_tokens_ids[train_size:]\n",
    "val_pt = portuguese_tokens_ids[train_size:]\n",
    "\n",
    "print(\"Size of training set: \", len(train_en))\n",
    "print(\"Size of validation set: \", len(val_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can create the Dataloader with the help of the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "list_data = [{'english':train_en[i],'portuguese':train_pt[i]} for i in range(len(train_en))]\n",
    "train_dataset = Dataset.from_list(list_data)\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "\n",
    "batch_size = 32 # number of sequences in each batch\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size) # train_dataloader is an iterator that returns a batch each time it is called\n",
    "\n",
    "list_data = [{'english':val_en[i],'portuguese':val_pt[i]} for i in range(len(val_en))]\n",
    "val_dataset = Dataset.from_list(list_data)\n",
    "val_dataset = val_dataset.with_format(\"torch\")\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size) # val_dataloader is an iterator that returns a batch each time it is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder, Decoder \n",
    "\n",
    "We are now ready to implement the encoder, decoder and seq2seq models. We will use a LSTM for both the encoder and the decoder.\n",
    "\n",
    "Let's start with the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5199ba1650>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main class of the RNN built from the nn.Module class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_d,hidden_d,n_layers, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the RNN Module\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: size of the vocabulary\n",
    "        output_size: size of the output layer\n",
    "        emb_d: size of the embedding layer\n",
    "        h_d: size of the hidden layer\n",
    "        n_layers: number of layers\n",
    "        drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_d)\n",
    "\n",
    "        # define a RNN layer\n",
    "        self.rnn = nn.GRU(emb_d, hidden_d, n_layers, dropout = drop_prob, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch_size\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "\n",
    "        Arguments:\n",
    "        x: input to the model\n",
    "        hidden: hidden state\n",
    "\n",
    "        Returns:\n",
    "        output: output of the model\n",
    "        hidden: hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # get the embedding vectors from lookup embedding layer \n",
    "        embeds = self.embedding(x) # shape: (batch_size, seq_length, emb_d)\n",
    "\n",
    "        # pass the embedding vectors to the RNN layer. We get the output and the hidden state and cell state to initialize the decoder  \n",
    "        # shape of out: (batch_size, seq_length, hidden_d)\n",
    "        # shape of hidden: (n_layers, batch_size, hidden_d)\n",
    "        # shape of cell: (n_layers, batch_size, hidden_d)\n",
    "        out, hidden = self.rnn(embeds) \n",
    "        out = self.dropout(out)\n",
    "        return out, hidden[-1:,:,:] # we return only the hidden state of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(11464, 50)\n",
       "  (rnn): GRU(50, 8, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokeng2id)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 8\n",
    "n_layers = 3\n",
    "\n",
    "model_enc = Encoder(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "model_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['let', \"'s\", 'try', 'something', '.']\n",
      "Tokens IDS: [6057, 24, 10606, 9529, 29]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\",english_tokens[0])\n",
    "token_ids = [tokeng2id[tk] for tk in english_tokens[0]]\n",
    "print(\"Tokens IDS:\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'have', 'to', 'go', 'to', 'sleep', '.']\n",
      "Tokens IDS: [5193, 4898, 10402, 4589, 10402]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\",english_tokens[2])\n",
    "token_ids_2 = [tokeng2id[tk] for tk in english_tokens[2]][:5]\n",
    "print(\"Tokens IDS:\",token_ids_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 8])\n",
      "Hidden shape: torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "model_enc.eval()\n",
    "output_enc,hidden_z = model_enc(torch.IntTensor([token_ids,token_ids_2])) # We had two sentences just to check simulate a batch of size 2\n",
    "print(\"Output shape:\",output_enc.shape)\n",
    "print(\"Hidden shape:\",hidden_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention class\n",
    "# calculates the attention weights and the context vector for each time step of the decoder, using the dot product of the last hidden state\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_d):\n",
    "        super().__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # permute the hidden dimensions to (batch_size, hidden_d, n_layers)\n",
    "        hidden = hidden.permute(1,2,0) \n",
    "\n",
    "        # dot product between encoder outputs and hidden state\n",
    "        # attention_weights shape: (batch_size, seq_length, 1)\n",
    "        attention_weights = torch.bmm(encoder_outputs,hidden)\n",
    "\n",
    "        # softmax to get the attention weights:\n",
    "        attention_weights = nn.functional.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # permute the attention weights to (batch_size, 1, seq_length) to do the weighted sum\n",
    "        context = torch.bmm(attention_weights.permute(0,2,1),encoder_outputs)\n",
    "\n",
    "        # sum along the seq_lengt axis to get the final context vector\n",
    "        context = context.sum(dim=1)\n",
    "        # add a dimension to the context vector to match the shape of the hidden state\n",
    "        context = context.unsqueeze(1)\n",
    "\n",
    "        return context, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define the decoder class\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_d,hidden_d, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the RNN Module\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: size of the vocabulary\n",
    "        output_size: size of the output layer\n",
    "        emb_d: size of the embedding layer\n",
    "        h_d: size of the hidden layer\n",
    "        drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_d)\n",
    "\n",
    "        # define a RNN layer\n",
    "        # To make things more simple, we will use only one layer\n",
    "        self.rnn = nn.GRU(emb_d + hidden_d, hidden_d, dropout = drop_prob, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch_size\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # define the output layer\n",
    "        self.fc = nn.Linear(hidden_d, vocab_size)\n",
    "\n",
    "        # define the attention layer\n",
    "        self.attention = Attention(hidden_d)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "\n",
    "        \"\"\"\n",
    "        Forward propagate through the RNN module\n",
    "\n",
    "        Arguments:\n",
    "        x: input to the RNN\n",
    "        hidden: hidden state\n",
    "        encoder_outputs: output of encoder\n",
    "        \"\"\"\n",
    "        # pass the input through the embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # get the attention weights and context\n",
    "        # context shape: (batch_size, 1, hidden_d)\n",
    "        # attention_weights shape: (batch_size, seq_length, 1)\n",
    "        context, attention_weights = self.attention(hidden,encoder_outputs)\n",
    "        \n",
    "        # concatenate the context and the embedded input\n",
    "        # rnn_input shape: (batch_size, 1, emb_d + hidden_d)    \n",
    "        rnn_input = torch.cat((x, context), dim=2)\n",
    "        \n",
    "        # pass the input and hidden state to the rnn\n",
    "        # output shape: (batch_size, 1, hidden_d)\n",
    "        # hidden shape: (n_layers, batch_size, hidden_d)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # pass the output through the output layer\n",
    "        # output shape: (batch_size, vocab_size)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden, attention_weights    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's check if the decoder is working using the context vectors we got from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(11464, 50)\n",
       "  (rnn): GRU(58, 8, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=8, out_features=11464, bias=True)\n",
       "  (attention): Attention()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dec = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "model_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with a forward pass on the first english example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1, 11464])\n",
      "Hidden shape: torch.Size([1, 2, 8])\n",
      "Attention weights shape: torch.Size([2, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "new_out,new_h,att_w = model_dec(torch.IntTensor([token_ids[-1:],token_ids[-1:]]),hidden_z,output_enc) # Simulate the input of the last token of the sentence\n",
    "print(\"Output shape:\",new_out.shape)\n",
    "print(\"Hidden shape:\",new_h.shape)\n",
    "print(\"Attention weights shape:\",att_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "We are now ready to implement the seq2seq model. The seq2seq model will take as input the source sequence and will output the predicted target sequence. \n",
    "\n",
    "Since we want to train with teacher forcing, we will pass the target sequence to the decoder. Teacher forcing is a technique where the target word is passed, with some probability, as the next input to the decoder. The intuition behind teacher forcing is that it will help the decoder learn to better predict the next token.\n",
    "\n",
    "We will use the Encoder and Decoder classes we implemented before. The encoder will take as input the source sequence and will output the context vectors (the last hidden and cell states). Then we will use the Decoder class to iterate over the target sequence and predict the next token. In each iteration we predict the next token only, based in the previous hidden can cell states, and the previous true or predicted token, depending on the teacher forcing probability. \n",
    "\n",
    "Follows the main steps we need to implement:\n",
    "\n",
    "1. Pass the source sequence to the encoder and get the context vectors.\n",
    "2. Initialize the decoder with the context vectors and the \\<sos\\> token.\n",
    "3. Predict the next token, hidden and cell states.\n",
    "4. Repeat 3. with the true or predicted token, depending on the teacher forcing probability, and the new hidden and cell states. \n",
    "5. Stop when we reach the maximum length of the sequence or when we predict the \\<eos\\> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = len(tokpt2id)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        # _, (hidden,cell) = self.encoder(source)\n",
    "        encoder_outputs, hidden = self.encoder(source)\n",
    "        # first input to the decoder is the <sos> token\n",
    "        # shape of x: (batch_size, 1)\n",
    "        x = target[:,:1]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "\n",
    "            output, hidden, att_w = self.decoder(x, hidden, encoder_outputs)\n",
    "\n",
    "            outputs[:,t:t+1,:] = output\n",
    "\n",
    "            best_guess = output.argmax(dim = -1)\n",
    "\n",
    "            x = target[:,t:t+1] if random.random() < teacher_forcing_ratio else best_guess\n",
    "\n",
    "        return outputs,att_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11464, 100)\n",
       "    (rnn): GRU(100, 256, batch_first=True, dropout=0.2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(17557, 100)\n",
       "    (rnn): GRU(356, 256, batch_first=True, dropout=0.2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (fc): Linear(in_features=256, out_features=17557, bias=True)\n",
       "    (attention): Attention()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_eng_size = len(tokeng2id)\n",
    "vocab_pt_size = len(tokpt2id)\n",
    "device = \"cuda\"\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "model_enc = Encoder(vocab_eng_size, embedding_dim, hidden_dim, n_layers)\n",
    "model_dec = Decoder(vocab_pt_size, embedding_dim, hidden_dim)\n",
    "\n",
    "model = Seq2Seq(model_enc, model_dec).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,160,745 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f0619847246ea8a276d6760b4f913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250 | Train Loss:  5.892 | Val Loss:  5.520 | Train Perplexity:  362.224 | Val Perplexity:  249.695\n",
      "Step: 500 | Train Loss:  5.586 | Val Loss:  5.232 | Train Perplexity:  266.778 | Val Perplexity:  187.167\n",
      "Step: 750 | Train Loss:  5.367 | Val Loss:  4.986 | Train Perplexity:  214.232 | Val Perplexity:  146.300\n",
      "Step: 1000 | Train Loss:  5.190 | Val Loss:  4.819 | Train Perplexity:  179.392 | Val Perplexity:  123.885\n",
      "Step: 1250 | Train Loss:  5.038 | Val Loss:  4.649 | Train Perplexity:  154.183 | Val Perplexity:  104.434\n",
      "Epochs: 1 | Train Loss:  5.038 | Val Loss:  4.649 | Train Perplexity:  154.183 | Val Perplexity:  104.434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8fab39bbf49ea9fc3b3d27e790d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500 | Train Loss:  4.086 | Val Loss:  4.524 | Train Perplexity:  59.500 | Val Perplexity:  92.188\n",
      "Step: 1750 | Train Loss:  4.014 | Val Loss:  4.436 | Train Perplexity:  55.364 | Val Perplexity:  84.473\n",
      "Step: 2000 | Train Loss:  3.942 | Val Loss:  4.349 | Train Perplexity:  51.497 | Val Perplexity:  77.415\n",
      "Step: 2250 | Train Loss:  3.887 | Val Loss:  4.267 | Train Perplexity:  48.763 | Val Perplexity:  71.274\n",
      "Step: 2500 | Train Loss:  3.832 | Val Loss:  4.224 | Train Perplexity:  46.168 | Val Perplexity:  68.272\n",
      "Epochs: 2 | Train Loss:  3.832 | Val Loss:  4.224 | Train Perplexity:  46.168 | Val Perplexity:  68.272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0fb4e8a70d4662b5b43bf0cdc06c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2750 | Train Loss:  3.222 | Val Loss:  4.175 | Train Perplexity:  25.083 | Val Perplexity:  65.019\n",
      "Step: 3000 | Train Loss:  3.196 | Val Loss:  4.146 | Train Perplexity:  24.442 | Val Perplexity:  63.182\n",
      "Step: 3250 | Train Loss:  3.193 | Val Loss:  4.094 | Train Perplexity:  24.359 | Val Perplexity:  59.957\n",
      "Step: 3500 | Train Loss:  3.172 | Val Loss:  4.078 | Train Perplexity:  23.849 | Val Perplexity:  59.052\n",
      "Step: 3750 | Train Loss:  3.152 | Val Loss:  4.024 | Train Perplexity:  23.391 | Val Perplexity:  55.918\n",
      "Epochs: 3 | Train Loss:  3.152 | Val Loss:  4.024 | Train Perplexity:  23.391 | Val Perplexity:  55.918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0944ab0bd94ce1bcc2e9670922b85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000 | Train Loss:  2.680 | Val Loss:  4.010 | Train Perplexity:  14.583 | Val Perplexity:  55.131\n",
      "Step: 4250 | Train Loss:  2.684 | Val Loss:  4.012 | Train Perplexity:  14.642 | Val Perplexity:  55.271\n",
      "Step: 4500 | Train Loss:  2.679 | Val Loss:  3.998 | Train Perplexity:  14.571 | Val Perplexity:  54.491\n",
      "Step: 4750 | Train Loss:  2.672 | Val Loss:  3.999 | Train Perplexity:  14.474 | Val Perplexity:  54.545\n",
      "Step: 5000 | Train Loss:  2.674 | Val Loss:  3.932 | Train Perplexity:  14.501 | Val Perplexity:  51.010\n",
      "Epochs: 4 | Train Loss:  2.674 | Val Loss:  3.932 | Train Perplexity:  14.501 | Val Perplexity:  51.010\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# main training loop\n",
    "n_epochs = 4\n",
    "lr=1e-3\n",
    "clip = 1\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "step = 0\n",
    "evaluation_step = 250\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train_total = 0\n",
    "\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        step += 1\n",
    "\n",
    "        source = batch['english'].to(device)\n",
    "        target = batch['portuguese'].to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output,_ = model(source, target)\n",
    "        output = output.to(device)\n",
    "\n",
    "        output = output[:,1:,:].reshape(-1, output.shape[2])\n",
    "        target = target[:,1:].reshape(-1)\n",
    "\n",
    "        # target = target.to(\"cpu\")\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss_train_total += loss.item()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradients to prevent exploding gradient problem. This step is very important.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "        # evaluation step\n",
    "        if step % evaluation_step == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # evaluate on training data\n",
    "                loss_val = 0\n",
    "                for j,batch in enumerate(val_dataloader):\n",
    "                    source = batch['english'].to(device)\n",
    "                    target = batch['portuguese'].to(device)\n",
    "\n",
    "                    # forward pass\n",
    "                    output,_ = model(source, target, teacher_forcing_ratio = 0) # we do not use teacher forcing here\n",
    "                    output = output.to(device)\n",
    "\n",
    "                    output = output[:,1:,:].reshape(-1, output.shape[2])\n",
    "                    target = target[:,1:].reshape(-1)\n",
    "\n",
    "                    loss = criterion(output, target)\n",
    "                    loss_val += loss.item()\n",
    "\n",
    "\n",
    "            # Calculate training perplexity\n",
    "            train_perplexity = math.exp(loss_train_total/i)\n",
    "\n",
    "\n",
    "            # Calculate perplexity\n",
    "            val_perplexity = math.exp(loss_val / j)\n",
    "\n",
    "\n",
    "            # print the loss at each step \n",
    "            print(f'Step: {step} | Train Loss: {loss_train_total/i: .3f} | Val Loss: {loss_val/j: .3f} | Train Perplexity: {train_perplexity: .3f} | Val Perplexity: {val_perplexity: .3f}')\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            train_losses.append(loss_train_total/i)\n",
    "            val_losses.append(loss_val/j)\n",
    "\n",
    "    \n",
    "    # print the loss and ppl at each epoch\n",
    "    print(f'Epochs: {epoch + 1} | Train Loss: {loss_train_total/i: .3f} | Val Loss: {loss_val/j: .3f} | Train Perplexity: {train_perplexity: .3f} | Val Perplexity: {val_perplexity: .3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now let's test it by generating a translation from english to portuguese, by building a function that receives a sentence in english and outputs the predicted translation in portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,max_len = 30):\n",
    "    tokens = word_tokenize(text.lower(), language='english')\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    tokens_ids = [tokeng2id[t] for t in tokens]\n",
    "    tokens_tensor = torch.LongTensor(tokens_ids).unsqueeze(0).to(device)\n",
    "    target_tensor = torch.LongTensor([1]*max_len).unsqueeze(0).to(device) # We just need a tensor with defined max length. Except the first token, the other tokens are not important, since we will not use teacher forcing.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions,_ = model(tokens_tensor,target_tensor,teacher_forcing_ratio=0)\n",
    "        for i in range(1,max_len):\n",
    "            predicted_id = predictions[0,i,:].argmax(dim=-1).item()\n",
    "            if predicted_id == 2: # <eos>   \n",
    "                break\n",
    "            print(id2tokpt[predicted_id],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi , onde você , onde você ? ? "
     ]
    }
   ],
   "source": [
    "translate(\"Hello, where do you live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi , o que é o seu nome é ? "
     ]
    }
   ],
   "source": [
    "translate(\"Hello, what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_w(text,max_len = 30):\n",
    "    tokens = word_tokenize(text.lower(), language='english')\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    print(tokens)\n",
    "    tokens_ids = [tokeng2id[t] for t in tokens]\n",
    "    tokens_tensor = torch.LongTensor(tokens_ids).unsqueeze(0).to(device)\n",
    "    target_tensor = torch.LongTensor([1]*max_len).unsqueeze(0).to(device) # We just need a tensor with defined max length. Except the first token, the other tokens are not important, since we will not use teacher forcing.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions,att_w = model(tokens_tensor,target_tensor,teacher_forcing_ratio=0)\n",
    "        return att_w\n",
    "        # for i in range(1,max_len):\n",
    "        #     predicted_id = predictions[0,i,:].argmax(dim=-1).item()\n",
    "        #     if predicted_id == 2: # <eos>   \n",
    "        #         break\n",
    "        #     print(id2tokpt[predicted_id],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'hello', ',', 'what', 'is', 'your', 'name', '?', '<eos>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9325e-06],\n",
       "         [2.1797e-11],\n",
       "         [4.5073e-13],\n",
       "         [3.6881e-10],\n",
       "         [1.0367e-08],\n",
       "         [1.8424e-08],\n",
       "         [1.6288e-05],\n",
       "         [9.2460e-02],\n",
       "         [9.0752e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_att_w(\"Hello, what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
