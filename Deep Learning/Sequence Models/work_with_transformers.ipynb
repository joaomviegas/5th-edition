{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to repeat the task of sentiment classification but this time using a transformers\n",
    "\n",
    "We will use the very known model BERT (Bidirectional Encoder Representations from Transformers) to perform the task of sentiment classification.\n",
    "\n",
    "The model is available in HuggingFace library, which is a library that provides state of the art pretrained models for NLP tasks.\n",
    "\n",
    "For this notebook we will use the model \"bert-base-case\", which is a model that has been trained on a large corpus of text in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We are going to use the same dataset as in the previous notebook: imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_imdb = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with huggingface, we need to use the tokenizer of the model we are going to use to tokenize the data.\n",
    "\n",
    "We are going to use the pretrained model bert-base-uncased for our classifier. \n",
    "\n",
    "We can use AutoTokenizer to automatically download the tokenizer of the model we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer from the model bert-base-cased is a WordPiece tokenizer, which means that it will split the words into subwords.\n",
    "\n",
    "It has already been trained on a large corpus of text and it has a fixed vocabulary.\n",
    "\n",
    "We can use the tokenizer to tokenize the data, convert the tokens to ids, add the special tokens [CLS] and [SEP] to the data, and pad the data to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'an', 'input', 'example']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('This is an input example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1188, 1110, 1126, 7758, 1859, 102]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('This is an input example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(101))\n",
    "print(tokenizer.convert_ids_to_tokens(102))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that now the tokens might not be words. They can be subwords or even characters. Let's see the tokens for the example we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1188, 1110, 1126, 7758, 1859,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('This is an input example', return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'This', 'is', 'an', 'input', 'example', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for tk_id in inputs.input_ids:\n",
    "    tk_str = tokenizer.convert_ids_to_tokens(tk_id)\n",
    "    print(tk_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sentence is already framed between the special tokens [CLS] and [SEP]. The tokenizer can also adds a special token for padding [PAD] and a special token for unknown words [UNK].\n",
    "We can see the special tokens already defined in the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      ".\n",
      "[SEP]\n",
      "[PAD]\n",
      "[PAD]\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is a test.\", truncation=True, max_length=10, padding=\"max_length\")\n",
    "for tk_id in inputs.input_ids:\n",
    "    tk_str = tokenizer.convert_ids_to_tokens(tk_id)\n",
    "    print(tk_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can create the main tokenizer train and test sets, by applying the tokenizer to all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c97f4cb07f844048401651af54e7612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dados_imdb = dados_imdb.map(tokenize_function, batched=True)\n",
    "dados_imdb = dados_imdb.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # number of sequences in each batch\n",
    "train_dataloader = DataLoader(dados_imdb['train'], shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(dados_imdb['test'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training\n",
    "\n",
    "We can now load the model. We are going to use BertModel from transformers library. But we can also use AutoModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-cased').to('cuda')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the last hidden cls state from the model and pass it to a linear layer to get the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = dados_imdb['train'][0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "                    input_ids=input['input_ids'].unsqueeze(0).to('cuda'), \n",
    "                    attention_mask=input['attention_mask'].unsqueeze(0).to('cuda'),\n",
    "                    output_hidden_states=True # we set this to True to get the hidden states\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting \"output_hidden_states\" to True, we get the hidden states of all the layers of the model. In this case 13 vectors, one for each layer, plus the input embeddings.\n",
    "\n",
    "We can use this to get the hidden state of the last layer and pass it to a general classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500, 768])\n",
      "tensor([[[ 0.4786, -0.2031, -0.3735,  ..., -0.2058,  0.1644,  0.2879],\n",
      "         [ 0.4285, -0.6476,  0.4748,  ..., -0.1825, -0.0327,  0.2230],\n",
      "         [ 0.5163,  0.1242,  0.0434,  ...,  0.4581, -0.0937, -0.0633],\n",
      "         ...,\n",
      "         [ 0.0302, -0.1422, -0.0431,  ...,  0.3726, -0.2235,  0.0446],\n",
      "         [-0.1261, -0.1953, -0.0077,  ...,  0.3068, -0.2196, -0.0858],\n",
      "         [-0.0613, -0.1139, -0.0212,  ...,  0.3339, -0.2237,  0.0156]]])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "print(output.hidden_states[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last hidden state is also available with the method \"last_hidden_state\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4786, -0.2031, -0.3735,  ..., -0.2058,  0.1644,  0.2879],\n",
       "         [ 0.4285, -0.6476,  0.4748,  ..., -0.1825, -0.0327,  0.2230],\n",
       "         [ 0.5163,  0.1242,  0.0434,  ...,  0.4581, -0.0937, -0.0633],\n",
       "         ...,\n",
       "         [ 0.0302, -0.1422, -0.0431,  ...,  0.3726, -0.2235,  0.0446],\n",
       "         [-0.1261, -0.1953, -0.0077,  ...,  0.3068, -0.2196, -0.0858],\n",
       "         [-0.0613, -0.1139, -0.0212,  ...,  0.3339, -0.2237,  0.0156]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.last_hidden_state.shape)\n",
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the last hidden state of the model to train a classifier. Let's try to train a simple Logistic Regression classifier on the last hidden state of the model.\n",
    "\n",
    "We start by get all vectors from the last hidden state of the model. We get the CLS token from the last hidden state and we use it as the representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:21<00:00, 15.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Let's start from the train dataset\n",
    "model.eval()\n",
    "last_h_list = []\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        output = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True # we set this to True to get the hidden states\n",
    "                    )\n",
    "        last_h = output.last_hidden_state[:,0,:].detach().cpu().numpy()\n",
    "        last_h_list.extend(last_h)\n",
    "last_h_train_array = np.array(last_h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_h_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:21<00:00, 15.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now the test dataset\n",
    "model.eval()\n",
    "last_h_list = []\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(tqdm(test_dataloader)):\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        output = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True # we set this to True to get the hidden states\n",
    "                    )\n",
    "        last_h = output.last_hidden_state[:,0,:].detach().cpu().numpy()\n",
    "        last_h_list.extend(last_h)\n",
    "last_h_test_array = np.array(last_h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_h_test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for train and test\n",
    "y_train = dados_imdb['train']['label'].detach().cpu().numpy()\n",
    "y_test = dados_imdb['test']['label'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a simple Logistic Regression classifier on the last hidden state of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## Exercise: Code a logistic regression model using the last hidden state of the BERT model\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model end-to-end. We are going to use the model BertForSequenceClassification, which is a BertModel with a classifier on top of it for sequence classification. We AutoModelForSequenceClassification to automatically download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2).to('cuda')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_imdb['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Trainer and TrainingArguments from the transformers library to train the model, that makes everything easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=epochs,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_ratio=0.01,                # number of warmup steps for learning rate scheduler\n",
    "    learning_rate = 1e-4,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='epoch',\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision = precision_score(labels, preds, average='micro')\n",
    "    recall = recall_score(labels, preds, average='micro')\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dados_imdb['train'],         # training dataset\n",
    "    eval_dataset=dados_imdb['test'],             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    # data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3128' max='3128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3128/3128 54:49, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.324059</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.880080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.270654</td>\n",
       "      <td>0.910480</td>\n",
       "      <td>0.910480</td>\n",
       "      <td>0.910480</td>\n",
       "      <td>0.910480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.292090</td>\n",
       "      <td>0.903880</td>\n",
       "      <td>0.903880</td>\n",
       "      <td>0.903880</td>\n",
       "      <td>0.903880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.314440</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.349181</td>\n",
       "      <td>0.917360</td>\n",
       "      <td>0.917360</td>\n",
       "      <td>0.917360</td>\n",
       "      <td>0.917360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.421547</td>\n",
       "      <td>0.919880</td>\n",
       "      <td>0.919880</td>\n",
       "      <td>0.919880</td>\n",
       "      <td>0.919880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3128, training_loss=0.1381366890867043, metrics={'train_runtime': 3290.5036, 'train_samples_per_second': 30.39, 'train_steps_per_second': 0.951, 'total_flos': 2.5694439e+16, 'train_loss': 0.1381366890867043, 'epoch': 4.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## Exercise: Code a prediction function that recieves a text and returns the predicted label\n",
    "#####\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review\n"
     ]
    }
   ],
   "source": [
    "predict(\"I enjoyed this movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative review\n"
     ]
    }
   ],
   "source": [
    "predict(\"This movie quite bad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
