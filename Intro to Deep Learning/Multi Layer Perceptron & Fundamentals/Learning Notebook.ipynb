{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in the previous section, we can use linear models to model non-linear data as long as we perform the right change of basis. However in practice we won't be able to tell exactly which change of basis we should do. That's where Deep Learning comes in. Let's start with the basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression (for a single input variable):\n",
    "$$ y = \\beta_1 x + \\beta_0 $$\n",
    "\n",
    "Linear Regression (for multiple input variables):\n",
    "$$ y = \\beta_n x_n + ... + \\beta_1 x_1 + \\beta_0 $$\n",
    "\n",
    "Same equation but with different notation:\n",
    "\n",
    "$$ f_{\\theta}(\\textbf{x}) = \\textbf{w}_{p\\times1}^T \\cdot \\textbf{x}_{p\\times1} + b_{1\\times1} $$\n",
    "\n",
    "where:\n",
    "- $\\theta$ are the model parameters: {$\\textbf{w}$ and $b$}\n",
    "- $ \\textbf{x}$ is a multi-variate input vector with shape (p, 1)\n",
    "- $ \\textbf{w}$ is a multi-variate parameter vector with shape (p, 1)\n",
    "\n",
    "A Linear Regression (for multiple input variables and multiple output variables):\n",
    "\n",
    "$$ \\textbf{f}_{\\theta}(\\textbf{x})_{q \\times 1} = \\textbf{W}_{q \\times p} \\cdot \\textbf{x}_{p\\times1}  + \\textbf{b}_{q \\times 1} $$\n",
    "\n",
    "What changes:\n",
    "1. The matrix $\\textbf{W}$ together with the bias vector $\\textbf{b}$ create $q$ linear regressions now instead of just one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of multi-output linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say i have some data with 2 dimensions: x and y. I'm interested in performing two linear regressions on this data.\n",
    "\n",
    "- The first one should be: $z_1 = 2x + y - 2$\n",
    "- The second one should be $z_2 = 5x - 3y$\n",
    "\n",
    "So for a given datapoint let's say $d_1 = (4,6)$, i would have:\n",
    "\n",
    "- $z_1 = 2\\times4 + 6 -2 = 12$\n",
    "- $z_2 = 5\\times4 - 3\\times 6 =2$\n",
    "\n",
    "\n",
    "Let's represent this in our previous notation:\n",
    "\n",
    "$$ \\textbf{f}_{\\theta}(\\textbf{x})_{q \\times 1} = \\textbf{W}_{q \\times p} \\cdot \\textbf{x}_{p\\times1}  + \\textbf{b}_{q \\times 1} $$\n",
    "\n",
    "For this problem, we have:\n",
    "\n",
    "$$ \\textbf{f}_{\\theta}(\\textbf{x})_{2 \\times 1} = \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 5 & -3 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a very good intuition about matrix and vector operations (linear algebra), I highly recommend the following [youtube playlist by 3blue1brown](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&si=l0sbGDQDoKO1K3AT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP model with a single hidden layer that regresses a single output value has the following expression:\n",
    "\n",
    "$$ f_{\\theta}(\\textbf{x})_{1 \\times 1} = \\textbf{w}_{2_{q\\times 1}}^T \\cdot \\phi(\\textbf{W}_{1_{q\\times p}} \\cdot \\textbf{x}_{p \\times1} + \\textbf{b}_{1_{q \\times 1}})_{q \\times 1} + \\textbf{b}_{2_{q \\times 1}} $$\n",
    "\n",
    "where $\\phi$ is a non-linear function, for example [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), [tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions)... these are also called [activation functions](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this model first does a basis regression on the original data:\n",
    "1. It performs q linear transformations\n",
    "2. on each of them passes the output through a non-linear function\n",
    "\n",
    "The output of this basis regression is what we previously called `x_new`. It's our new **representation** of the input.\n",
    "\n",
    "Then the model simple performs a linear regression between `x_new` and the original target. Hopefully, `x_new` will help us perform a better linear regression on the target values than the original $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar thing will happen here, first we transform the original data into some new representation (basis) and then we perform Logistic Regression `x_new` as if it were out original input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{\\theta}(\\textbf{x})_{1 \\times 1} = \\text{Sigmoid}(\\textbf{w}_{2_{q\\times 1}}^T \\cdot \\phi(\\textbf{W}_{1_{q\\times p}} \\cdot \\textbf{x}_{p \\times1} + \\textbf{b}_{1_{q \\times 1}})_{q \\times 1} + \\textbf{b}_{2_{q \\times 1}}) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/1200px-Neural_network_example.svg.png\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A feedforward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function.\n",
    "\n",
    "The activation function is required to be a non-constant, bounded, and continuous function, making common activation functions like sigmoid, hyperbolic tangent (tanh), and Rectified Linear Unit (ReLU) suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Unfortunately theory is different from practice... check the Practical Wisdom section in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Neural Networks Learn: Backpropagation & Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the samsung Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Wisdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Shallow vs. Deep Neural Networks\n",
    "\n",
    "In practice it is a lot better to train `Deep` Neural Networks (+ layers, - neurons) than `Shallow` DNNs (- layers, + neurons). There are a few reasons why:\n",
    "\n",
    "### Expressiveness:\n",
    "- **Hierarchical Feature Learning**: Deep networks learn a hierarchy of features. Lower layers might learn simple patterns, and as you go up the layers, the network learns more complex and abstract representations. This hierarchical learning is crucial for tasks like image recognition, where you might want to detect edges in the first layer, shapes in the second layer, and more complex structures in subsequent layers.\n",
    "- **Compact Representations**: Deep networks can represent certain functions much more compactly than shallow networks. For a shallow network to learn the same function, it might require an exponentially larger number of neurons, making it computationally infeasible.\n",
    "\n",
    "### Training Efficiency:\n",
    "- **Parameter Efficiency**: Deep networks tend to use parameters more efficiently. They can learn with fewer total parameters compared to a single-layer network with a similar capacity.\n",
    "- **Vanishing/Exploding Gradients**: Though deep networks are also susceptible to vanishing and exploding gradients, extremely wide single-layer networks can also suffer from these issues, and they lack the structural benefits of depth.\n",
    "\n",
    "### Generalization and overfitting:\n",
    "- **Better Generalization**: Deep networks often generalize better to unseen data. This is crucial for the performance of the model on real-world tasks.\n",
    "\n",
    "### Optimization Landscape:\n",
    "- **Easier Optimization**: Training deep networks is a non-convex optimization problem, but the structure imposed by having multiple layers seems to make the optimization landscape easier to navigate, leading to better convergence properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization strategies\n",
    "\n",
    "Choosing the right step size (learning rate) is not trivial. \n",
    "\n",
    "- If the step size is too small, the model might get stuck in a local minimum or take a very long time to converge. \n",
    "- If it's too large, the model might overshoot the minimum or fail to converge entirely.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "It is true that we want to minimize error on the train set, but optimizing for the whole dataset at once is not preferable.\n",
    "\n",
    "In `SGD`, updates are made after computing the gradient on each individual data point.\n",
    "\n",
    "Why is `SGD` better than `Batch Gradient Descent`?\n",
    "\n",
    "1. Computationally efficient: doesn't require as much ram\n",
    "2. Ability to Escape Local minima: The noisy gradient estimates can actually be beneficial, as they can help the optimizer escape from local minima\n",
    "3. Convergence Speed: parameters are updated more regularly rather than in big steps\n",
    "3. Less sensitive to redundant and correlated data: because each update is based on a single data point.\n",
    "4. Generalization: The noisy updates can lead to better generalization on the test set.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Will generally be preferred to Stochatic Gradient Descent.\n",
    "\n",
    "Why?\n",
    "\n",
    "1. Training stability: Updates are computed based on a subset of the data, leading to less noisy and more stable updates than SGD which has high variance between updates.\n",
    "2. Computationally efficient: The use of vectorized operations makes computation more efficient.\n",
    "\n",
    "\n",
    "Now there are some addons you can further apply for the optimization:\n",
    "\n",
    "## Momentum\n",
    "\n",
    "Think of this as an addon that helps accelerate SGD or Mini-batch GD in the relevant direction and dampens oscillations. It does this by adding a fraction of the previous update vector to the current update vector.\n",
    "\n",
    "- It can help overcome local minima and oscillations, providing smoother convergence.\n",
    "- But must be carefully tuned\n",
    "\n",
    "### Gradient Clipping\n",
    "\n",
    "This method involves setting a threshold to limit the size of the gradients during backpropagation, preventing the exploding gradient problem and helping to ensure stable optimization.\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "\n",
    "The learning rate is reduced according to a pre-defined schedule or when the training plateaus. Common strategies include :\n",
    "1. step decay\n",
    "2. exponential decay\n",
    "3. or reducing the learning rate when the validation error stops improving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This playground let's you play with neural networks and visualize what's happening in a very cool way](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.00943&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
