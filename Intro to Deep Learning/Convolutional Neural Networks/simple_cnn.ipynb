{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from torchmetrics import Recall, Precision, F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader Objects for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_file, label_file, transform=None):\n",
    "        # Load the data and labels from the CSV files\n",
    "        self.data = np.loadtxt(data_file, delimiter=',')\n",
    "        self.labels = np.loadtxt(label_file, delimiter=',', dtype=np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert the numpy arrays to PyTorch tensors\n",
    "        image = torch.tensor(self.data[index], dtype=torch.float32).reshape(1, 28, 28)  # reshaping to 1x28x28\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.int64)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "X_train_path = os.path.join('data/MNIST_Original/mnist_train_data.csv')\n",
    "y_train_path = os.path.join('data/MNIST_Original/mnist_train_labels.csv')\n",
    "X_test_path = os.path.join('data/MNIST_Original/mnist_test_data.csv')\n",
    "y_test_path = os.path.join('data/MNIST_Original/mnist_test_labels.csv')\n",
    "\n",
    "train_dataset = MNISTDataset(X_train_path, y_train_path)\n",
    "test_dataset = MNISTDataset(X_test_path, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # you can change this value based on your requirements\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Datasets\n",
    "\n",
    "Represents a Dataset in pytorch. Defines `__getitem__()` and `__len__()`.\n",
    "- `__getitem__()`: How to access a single data point\n",
    "- `__len__()`: size of the dataset\n",
    "\n",
    "## Torch DataLoader\n",
    "\n",
    "Wraps a `Dataset` object and provides utilities for batching, shuffling, and parallel data loading. \n",
    "\n",
    "It abstracts the complexity of batching and shuffling, providing a clean and efficient way to loop through your dataset in manageable chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what an example of a batch looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(trainloader))\n",
    "X, y = batch_data[0], batch_data[1]\n",
    "\n",
    "X.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is a matrix with 4 dimensions!\n",
    "\n",
    "1. 1st dimension (4): number of images in this batch\n",
    "2. 2nd dimension (1): Number of color channels (only one because images are in grey scale)\n",
    "3. 3rd dimension (28): height of image, in pixels\n",
    "4. 4th dimension (28): width of image, in pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_show = X[0].squeeze()  # Remove the channel dimension, resulting in [28, 28]\n",
    "plt.imshow(image_to_show, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty cool right? Let's make this into a function for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image : torch.Tensor):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our first CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # to capture basic patterns from the image\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)# to capture basic patterns from the previous patterns (results in capturing more complex patterns from the original image)\n",
    "        self.fc1 = nn.Linear(320, 50) # 320 = 20 * 4 * 4\n",
    "        self.fc2 = nn.Linear(50, 10) # DNN > WNN; also 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way we can visualize this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 28, 28)  # A dummy input tensor to pass through the model\n",
    "y = model(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "dot.render('model details/model_visualization', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Step 2: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(list(range(num_epochs))):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to interrupt training if you feel like the model is not improving anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points:\n",
    "\n",
    "1. Why are we only evaluating every 2000th batch?\n",
    "    - Because it takes time to evaluate a model, if we compute the loss everytime we update the model it would take a lot longer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluation\n",
    "\n",
    "# What's the loss on the train set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(trainloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the train set: %.3f' % (sum(total_loss) / len(total_loss)))\n",
    "\n",
    "\n",
    "# What's the loss on the test set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the test set: %.3f' % (sum(total_loss) / len(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok might be overfitting a bit but it's not too bad. Now let's look at more interesting metrics like recall and precision.\n",
    "\n",
    "For these metrics we require the model decisions, but our current model returns only the logits (the output of the last Linear Layer). So let's run this over a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels)\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to save a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models is very useful:\n",
    "1. share with others\n",
    "2. don't have to train again next time\n",
    "3. Create model checkpoints to:\n",
    "    - prevent loss from interruption\n",
    "    - limited access to computing resources\n",
    "    - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "torch.save(model.state_dict(), 'models/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pytorch model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('models/model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is this model still good at prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels)\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could package some of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_evaluation(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)  # forward\n",
    "            loss = criterion(outputs, labels)  # calculate loss\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            recall.update(predictions, labels)\n",
    "            precision.update(predictions, labels)\n",
    "            f1.update(predictions, labels)\n",
    "\n",
    "    average_loss = sum(total_loss) / len(total_loss)\n",
    "                \n",
    "    return {'loss':average_loss, \n",
    "            'recall':recall.compute().item(), \n",
    "            'precision':precision.compute().item(), \n",
    "            'f1':f1.compute().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_evaluation(model,testloader,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, print_every=2000):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "    running_loss = []\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "\n",
    "        running_loss.append(loss)\n",
    "\n",
    "        if i % print_every == print_every-1:  # print every 2000 mini-batches\n",
    "            print('[%5d] loss: %.3f' % (i + 1, loss.item()))\n",
    "    return torch.mean(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_epoch(model, trainloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in tqdm(list(range(2))):\n",
    "    train_one_epoch(model, trainloader, criterion, optimizer)\n",
    "    evaluation = dataloader_evaluation(model, testloader, criterion)\n",
    "    print('Epoch %d: loss=%.3f, recall=%.2f, precision=%.2f, f1=%.2f' % (epoch+1, evaluation['loss'], evaluation['recall'], evaluation['precision'], evaluation['f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
