{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from torchmetrics import Recall, Precision, F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader Objects for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_file, label_file, transform=None):\n",
    "        # Load the data and labels from the CSV files\n",
    "        self.data = np.loadtxt(data_file, delimiter=',')\n",
    "        self.labels = np.loadtxt(label_file, delimiter=',', dtype=np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert the numpy arrays to PyTorch tensors\n",
    "        image = torch.tensor(self.data[index], dtype=torch.float32).reshape(1, 28, 28)  # reshaping to 1x28x28, 1 canal, comprimento e altura em pixeis \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.int64)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoaoViegas\\AppData\\Local\\Temp\\ipykernel_12416\\3920772127.py:5: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
      "    * make sure the original data is stored as integers.\n",
      "    * use the `converters=` keyword argument.  If you only use\n",
      "      NumPy 1.23 or later, `converters=float` will normally work.\n",
      "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
      "      floating point and then convert it.  (On all NumPy versions.)\n",
      "  (Deprecated NumPy 1.23)\n",
      "  self.labels = np.loadtxt(label_file, delimiter=',', dtype=np.int64)\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data\n",
    "X_train_path = os.path.join('data/MNIST_Original/mnist_train_data.csv')\n",
    "y_train_path = os.path.join('data/MNIST_Original/mnist_train_labels.csv')\n",
    "X_test_path = os.path.join('data/MNIST_Original/mnist_test_data.csv')\n",
    "y_test_path = os.path.join('data/MNIST_Original/mnist_test_labels.csv')\n",
    "\n",
    "train_dataset = MNISTDataset(X_train_path, y_train_path)\n",
    "test_dataset = MNISTDataset(X_test_path, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # you can change this value based on your requirements\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) #é indiferente o shuffle estar a true ou false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Datasets\n",
    "\n",
    "Represents a Dataset in pytorch. Defines `__getitem__()` and `__len__()`.\n",
    "- `__getitem__()`: How to access a single data point\n",
    "- `__len__()`: size of the dataset\n",
    "\n",
    "## Torch DataLoader\n",
    "\n",
    "Wraps a `Dataset` object and provides utilities for batching, shuffling, and parallel data loading. \n",
    "\n",
    "It abstracts the complexity of batching and shuffling, providing a clean and efficient way to loop through your dataset in manageable chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what an example of a batch looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 1, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = next(iter(trainloader))\n",
    "X, y = batch_data[0], batch_data[1]\n",
    "\n",
    "X.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is a matrix with 4 dimensions!\n",
    "\n",
    "1. 1st dimension (4): number of images in this batch\n",
    "2. 2nd dimension (1): Number of color channels (only one because images are in grey scale)\n",
    "3. 3rd dimension (28): height of image, in pixels\n",
    "4. 4th dimension (28): width of image, in pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ef5b848430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_to_show = X[0].squeeze()  # Remove the channel dimension, resulting in [28, 28]\n",
    "plt.imshow(image_to_show, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty cool right? Let's make this into a function for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image : torch.Tensor):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our first CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # to capture basic patterns from the image\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)# to capture basic patterns from the previous patterns (results in capturing more complex patterns from the original image)\n",
    "        self.fc1 = nn.Linear(320, 50) # 320 = 20 * 4 * 4\n",
    "        self.fc2 = nn.Linear(50, 10) # DNN > WNN; also 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #return F.log_softmax(x, dim=1) #para a soma de todos os componentes dar 1, basicamente é normalizar\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way we can visualize this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model details\\\\model_visualization.png'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28)  # A dummy input tensor to pass through the model\n",
    "y = model(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "dot.render('model details/model_visualization', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9af099c2f74abeb206eca5735a0985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.787\n",
      "[1,  4000] loss: 0.599\n",
      "[1,  6000] loss: 0.463\n",
      "[1,  8000] loss: 0.430\n",
      "[1, 10000] loss: 0.445\n",
      "[1, 12000] loss: 0.754\n",
      "[1, 14000] loss: 1.160\n",
      "[2,  2000] loss: 1.207\n",
      "[2,  4000] loss: 0.773\n",
      "[2,  6000] loss: 1.402\n",
      "[2,  8000] loss: 1.843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JoaoViegas\\OneDrive - FCT NOVA\\curso_ai\\5th-edition\\Intro to Deep Learning\\Convolutional Neural Networks\\simple_cnn.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoaoViegas/OneDrive%20-%20FCT%20NOVA/curso_ai/5th-edition/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# backward\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoaoViegas/OneDrive%20-%20FCT%20NOVA/curso_ai/5th-edition/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# optimize\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JoaoViegas/OneDrive%20-%20FCT%20NOVA/curso_ai/5th-edition/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoaoViegas/OneDrive%20-%20FCT%20NOVA/curso_ai/5th-edition/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1999\u001b[39m:  \u001b[39m# print every 2000 mini-batches\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoaoViegas/OneDrive%20-%20FCT%20NOVA/curso_ai/5th-edition/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%5d\u001b[39;00m\u001b[39m] loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, running_loss \u001b[39m/\u001b[39m \u001b[39m2000\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 1: Define the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Step 2: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #usar este ou o Adam (otimiza o step size aka learning rate?? e torna-o variavel)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(list(range(num_epochs))):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to interrupt training if you feel like the model is not improving anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points:\n",
    "\n",
    "1. Why are we only evaluating every 2000th batch?\n",
    "    - Because it takes time to evaluate a model, if we compute the loss everytime we update the model it would take a lot longer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65351ada77de44669e8f66bcce19aa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on the train set: 1.689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719d2633c0a64ac3800f6fdbe05c66ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on the test set: 1.672\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluation\n",
    "\n",
    "# What's the loss on the train set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(trainloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the train set: %.3f' % (sum(total_loss) / len(total_loss)))\n",
    "\n",
    "\n",
    "# What's the loss on the test set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the test set: %.3f' % (sum(total_loss) / len(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok might be overfitting a bit but it's not too bad. Now let's look at more interesting metrics like recall and precision.\n",
    "\n",
    "For these metrics we require the model decisions, but our current model returns only the logits (the output of the last Linear Layer). So let's run this over a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e88d99cf1d4620b9253653a8fb23a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on the test set: 0.41\n",
      "Precision on the test set: 0.41\n",
      "F1 Score on the test set: 0.41\n"
     ]
    }
   ],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        #optimizer.zero_grad()  # zero the parameter gradients (redundante aqui)\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels) #update guarda os valores anteriores e os novos\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to save a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models is very useful:\n",
    "1. share with others\n",
    "2. don't have to train again next time\n",
    "3. Create model checkpoints to:\n",
    "    - prevent loss from interruption\n",
    "    - limited access to computing resources\n",
    "    - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "torch.save(model.state_dict(), 'models/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pytorch model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('models/model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is this model still good at prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b79fd647d94d7c9618bb40414091c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on the test set: 0.41\n",
      "Precision on the test set: 0.41\n",
      "F1 Score on the test set: 0.41\n"
     ]
    }
   ],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels)\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could package some of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_evaluation(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    recall = Recall(task='multiclass',num_classes=10)\n",
    "    precision = Precision(task='multiclass',num_classes=10)\n",
    "    f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)  # forward\n",
    "            loss = criterion(outputs, labels)  # calculate loss\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            recall.update(predictions, labels)\n",
    "            precision.update(predictions, labels)\n",
    "            f1.update(predictions, labels)\n",
    "\n",
    "    average_loss = sum(total_loss) / len(total_loss)\n",
    "                \n",
    "    return {'loss':average_loss, \n",
    "            'recall':recall.compute().item(), \n",
    "            'precision':precision.compute().item(), \n",
    "            'f1':f1.compute().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9947a7a4e52e4966a9cdcaa49e579625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.671946665647061,\n",
       " 'recall': 0.4101000130176544,\n",
       " 'precision': 0.4101000130176544,\n",
       " 'f1': 0.4101000130176544}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_evaluation(model,testloader,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, print_every=2000):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "    running_loss = []\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "\n",
    "        running_loss.append(loss)\n",
    "\n",
    "        if i % print_every == print_every-1:  # print every 2000 mini-batches\n",
    "            print('[%5d] loss: %.3f' % (i + 1, loss.item()))\n",
    "    return torch.mean(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b432921e4b994d9ab94927b2dd580b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2000] loss: 2.212\n",
      "[ 4000] loss: 1.102\n",
      "[ 6000] loss: 2.494\n",
      "[ 8000] loss: 0.642\n",
      "[10000] loss: 0.977\n",
      "[12000] loss: 1.979\n",
      "[14000] loss: 2.326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5875835418701172"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(model, trainloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aefa003f87f4679a69aa5ddefe08d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb45a52bfa4448489ede581a19cc2bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2000] loss: 0.067\n",
      "[ 4000] loss: 0.222\n",
      "[ 6000] loss: 0.546\n",
      "[ 8000] loss: 0.594\n",
      "[10000] loss: 3.314\n",
      "[12000] loss: 0.602\n",
      "[14000] loss: 0.027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16640156507492065"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d003642198cc4643ae9a01b76b0479f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.589, recall=0.85, precision=0.85, f1=0.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f770bf932a434e1f82254d4b8df8d1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2000] loss: 0.631\n",
      "[ 4000] loss: 2.308\n",
      "[ 6000] loss: 2.278\n",
      "[ 8000] loss: 2.251\n",
      "[10000] loss: 2.324\n",
      "[12000] loss: 2.306\n",
      "[14000] loss: 2.265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3326752185821533"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ce6213201b42a0bb95d08fdd73d257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=2.302, recall=0.12, precision=0.12, f1=0.12\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in tqdm(list(range(2))):\n",
    "    train_one_epoch(model, trainloader, criterion, optimizer)\n",
    "    evaluation = dataloader_evaluation(model, testloader, criterion)\n",
    "    print('Epoch %d: loss=%.3f, recall=%.2f, precision=%.2f, f1=%.2f' % (epoch+1, evaluation['loss'], evaluation['recall'], evaluation['precision'], evaluation['f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
