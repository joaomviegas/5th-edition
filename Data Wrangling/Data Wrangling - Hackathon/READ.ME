# Description of the dataset:

The provided dataset serves as a valuable resource for addressing inquiries related to non-pharmaceutical interventions (NPIs) concerning epidemiological outbreaks of the novel coronavirus (COVID-19) in different USA counties. It is a machine-readable compilation of data encompassing socioeconomic factors that possess the potential to influence the propagation and consequences of COVID-19 outbreaks. The dataset includes time-series data spanning from January to March, 2020, encompassing information on infections and fatalities attributed to COVID-19, as well as foot traffic patterns at various points of interest, categorized by type and aggregated at the county level. The primary objective is to investigate potential variations in the effectiveness of NPIs across different counties and ascertain whether such variances can be anticipated based on county-specific characteristics.

The dataset was meticulously curated by amalgamating data from diverse sources. To ensure data clarity and utility, only information that had been transformed into a machine-readable format was incorporated. 

It is envisaged that this dataset will be a valuable asset for the data science, machine learning, and epidemiological modeling communities, facilitating research and analysis in the domain of COVID-19 and NPIs.

ArXiv report on dataset: http://arxiv.org/abs/2004.00756



# Objectives:

In this hackathon we are health safety inspectors that are in the middle of the covid, and you need to make fast decisions on the future of the population, amist the caos. For us to engage people into making better choises, we really need to convey the information of what is happening!

For this you have to organize, combine and analyse the dataset and present graphics so that people could understand what happened and what could they do to prevent the virus from spreading. You can focus on what details you see fit, but we want to grasp what happened during that period. (Have fun!)

# Guide for a successfull hackathon

For you to accomplish this, you should to:
  1. Load the dataset files into a pandas dataframe (17:45 - 18:00)
  2. Combine the datasets (you don't have to combine them all) using join() (18:00 - 18:15)
  3. Understand the data (18:15-18:30)
  4. Make some statistics on what you think is relevant to show (18:30-19:00)
  5. Make appealing graphics to explain the data that you focused on (using barplots, plots, scatter, etc...) and make some correlations (e.g. using pairplot) that seems reasonable (19:00-20:00)